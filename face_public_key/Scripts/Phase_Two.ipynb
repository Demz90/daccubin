{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/dimensionality_reduction/base_reducer.py\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DimensionalityReducer(ABC):\n",
    "    \"\"\"Base class for all dimensionality reduction techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, target_dims=32, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the reducer\n",
    "        \n",
    "        Args:\n",
    "            target_dims: Target dimensionality for reduction\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.target_dims = target_dims\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, embeddings):\n",
    "        \"\"\"\n",
    "        Fit the dimensionality reduction model\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform(self, embeddings):\n",
    "        \"\"\"\n",
    "        Transform embeddings to lower dimension\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            reduced_embeddings: Array of reduced embeddings [n_samples, target_dims]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit_transform(self, embeddings):\n",
    "        \"\"\"\n",
    "        Fit and transform in one step\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            reduced_embeddings: Array of reduced embeddings [n_samples, target_dims]\n",
    "        \"\"\"\n",
    "        self.fit(embeddings)\n",
    "        return self.transform(embeddings)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the fitted model\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model is not fitted yet\")\n",
    "        \n",
    "        # Implement in subclasses\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"\n",
    "        Load a fitted model\n",
    "        \n",
    "        Args:\n",
    "            path: Path to load the model from\n",
    "            \n",
    "        Returns:\n",
    "            reducer: Loaded dimensionality reducer\n",
    "        \"\"\"\n",
    "        # Implement in subclasses\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/dimensionality_reduction/pca_reducer.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "from key_generation.dimensionality_reduction.base_reducer import DimensionalityReducer\n",
    "\n",
    "class PCAReducer(DimensionalityReducer):\n",
    "    \"\"\"PCA-based dimensionality reduction\"\"\"\n",
    "    \n",
    "    def __init__(self, target_dims=32, random_state=42, whiten=True):\n",
    "        \"\"\"\n",
    "        Initialize PCA reducer\n",
    "        \n",
    "        Args:\n",
    "            target_dims: Target dimensionality for reduction\n",
    "            random_state: Random seed for reproducibility\n",
    "            whiten: Whether to whiten the data (decorrelate features)\n",
    "        \"\"\"\n",
    "        super().__init__(target_dims=target_dims, random_state=random_state)\n",
    "        self.whiten = whiten\n",
    "    \n",
    "    def fit(self, embeddings):\n",
    "        \"\"\"\n",
    "        Fit PCA model to embeddings\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        # Initialize the PCA model\n",
    "        self.model = PCA(\n",
    "            n_components=self.target_dims,\n",
    "            whiten=self.whiten,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        self.model.fit(embeddings)\n",
    "        \n",
    "        # Calculate and store explained variance\n",
    "        self.explained_variance_ratio = self.model.explained_variance_ratio_\n",
    "        self.cumulative_variance = np.cumsum(self.explained_variance_ratio)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, embeddings):\n",
    "        \"\"\"\n",
    "        Transform embeddings to lower dimension using PCA\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            reduced_embeddings: Array of reduced embeddings [n_samples, target_dims]\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"PCA model is not fitted yet\")\n",
    "        \n",
    "        reduced_embeddings = self.model.transform(embeddings)\n",
    "        return reduced_embeddings\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the fitted PCA model\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"PCA model is not fitted yet\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        joblib.dump(self.model, path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"\n",
    "        Load a fitted PCA model\n",
    "        \n",
    "        Args:\n",
    "            path: Path to load the model from\n",
    "            \n",
    "        Returns:\n",
    "            reducer: Loaded PCA reducer\n",
    "        \"\"\"\n",
    "        # Load model\n",
    "        model = joblib.load(path)\n",
    "        \n",
    "        # Create reducer instance\n",
    "        reducer = cls(\n",
    "            target_dims=model.n_components,\n",
    "            random_state=model.random_state,\n",
    "            whiten=model.whiten\n",
    "        )\n",
    "        \n",
    "        # Restore model state\n",
    "        reducer.model = model\n",
    "        reducer.explained_variance_ratio = model.explained_variance_ratio_\n",
    "        reducer.cumulative_variance = np.cumsum(reducer.explained_variance_ratio)\n",
    "        reducer.is_fitted = True\n",
    "        \n",
    "        return reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/dimensionality_reduction/autoencoder_reducer.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from key_generation.dimensionality_reduction.base_reducer import DimensionalityReducer\n",
    "\n",
    "class AutoencoderReducer(DimensionalityReducer):\n",
    "    \"\"\"Autoencoder-based dimensionality reduction\"\"\"\n",
    "\n",
    "    def __init__(self, target_dims=32, random_state=42, hidden_layer_sizes=None, epochs=50, batch_size=32, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initialize Autoencoder reducer\n",
    "\n",
    "        Args:\n",
    "            target_dims: Target dimensionality for reduction\n",
    "            random_state: Random seed for reproducibility\n",
    "            hidden_layer_sizes: List of hidden layer sizes for the encoder\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Batch size for training\n",
    "            learning_rate: Learning rate for the optimizer\n",
    "        \"\"\"\n",
    "        super().__init__(target_dims=target_dims, random_state=random_state)\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes or [128, 64]\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.encoder = None\n",
    "        self.autoencoder = None\n",
    "\n",
    "    def _build_autoencoder(self, input_dim):\n",
    "        \"\"\"\n",
    "        Build the autoencoder model\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimensionality of the input data\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        \n",
    "        # Encoder\n",
    "        x = input_layer\n",
    "        for size in self.hidden_layer_sizes:\n",
    "            x = Dense(size, activation='relu')(x)\n",
    "        encoded = Dense(self.target_dims, activation='relu')(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = encoded\n",
    "        for size in reversed(self.hidden_layer_sizes):\n",
    "            x = Dense(size, activation='relu')(x)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(x)\n",
    "        \n",
    "        # Autoencoder and encoder models\n",
    "        self.autoencoder = Model(input_layer, decoded)\n",
    "        self.encoder = Model(input_layer, encoded)\n",
    "        \n",
    "        # Compile the autoencoder\n",
    "        self.autoencoder.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "\n",
    "    def fit(self, embeddings):\n",
    "        \"\"\"\n",
    "        Fit the autoencoder model to embeddings\n",
    "\n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        input_dim = embeddings.shape[1]\n",
    "        self._build_autoencoder(input_dim)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        self.autoencoder.fit(\n",
    "            embeddings, embeddings,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        self.is_fitted = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, embeddings):\n",
    "        \"\"\"\n",
    "        Transform embeddings to lower dimension using the encoder\n",
    "\n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            reduced_embeddings: Array of reduced embeddings [n_samples, target_dims]\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Autoencoder model is not fitted yet\")\n",
    "        \n",
    "        reduced_embeddings = self.encoder.predict(embeddings)\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the fitted autoencoder model\n",
    "\n",
    "        Args:\n",
    "            path: Path to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Autoencoder model is not fitted yet\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.autoencoder.save(path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"\n",
    "        Load a fitted autoencoder model\n",
    "\n",
    "        Args:\n",
    "            path: Path to load the model from\n",
    "            \n",
    "        Returns:\n",
    "            reducer: Loaded Autoencoder reducer\n",
    "        \"\"\"\n",
    "        # Load the autoencoder model\n",
    "        autoencoder = load_model(path)\n",
    "        \n",
    "        # Extract the encoder part\n",
    "        input_layer = autoencoder.input\n",
    "        encoded_layer = autoencoder.layers[len(autoencoder.layers) // 2].output\n",
    "        encoder = Model(input_layer, encoded_layer)\n",
    "        \n",
    "        # Create reducer instance\n",
    "        reducer = cls(\n",
    "            target_dims=encoded_layer.shape[-1],\n",
    "            random_state=None  # Random state is not needed for a loaded model\n",
    "        )\n",
    "        reducer.autoencoder = autoencoder\n",
    "        reducer.encoder = encoder\n",
    "        reducer.is_fitted = True\n",
    "        \n",
    "        return reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/dimensionality_reduction/random_projection_reducer.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "import joblib\n",
    "\n",
    "from key_generation.dimensionality_reduction.base_reducer import DimensionalityReducer\n",
    "\n",
    "class RandomProjectionReducer(DimensionalityReducer):\n",
    "    \"\"\"Random Projection-based dimensionality reduction\"\"\"\n",
    "\n",
    "    def __init__(self, target_dims=32, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize Random Projection reducer\n",
    "\n",
    "        Args:\n",
    "            target_dims: Target dimensionality for reduction\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        super().__init__(target_dims=target_dims, random_state=random_state)\n",
    "\n",
    "    def fit(self, embeddings):\n",
    "        \"\"\"\n",
    "        Fit Random Projection model to embeddings\n",
    "\n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        # Initialize the Random Projection model\n",
    "        self.model = GaussianRandomProjection(\n",
    "            n_components=self.target_dims,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Fit the model (Random Projection doesn't require explicit fitting)\n",
    "        self.model.fit(embeddings)\n",
    "        self.is_fitted = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, embeddings):\n",
    "        \"\"\"\n",
    "        Transform embeddings to lower dimension using Random Projection\n",
    "\n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            reduced_embeddings: Array of reduced embeddings [n_samples, target_dims]\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Random Projection model is not fitted yet\")\n",
    "        \n",
    "        reduced_embeddings = self.model.transform(embeddings)\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the fitted Random Projection model\n",
    "\n",
    "        Args:\n",
    "            path: Path to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Random Projection model is not fitted yet\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        joblib.dump(self.model, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"\n",
    "        Load a fitted Random Projection model\n",
    "\n",
    "        Args:\n",
    "            path: Path to load the model from\n",
    "            \n",
    "        Returns:\n",
    "            reducer: Loaded Random Projection reducer\n",
    "        \"\"\"\n",
    "        # Load model\n",
    "        model = joblib.load(path)\n",
    "        \n",
    "        # Create reducer instance\n",
    "        reducer = cls(\n",
    "            target_dims=model.n_components,\n",
    "            random_state=model.random_state\n",
    "        )\n",
    "        \n",
    "        # Restore model state\n",
    "        reducer.model = model\n",
    "        reducer.is_fitted = True\n",
    "        \n",
    "        return reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f77c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/dimensionality_reduction/umap_reducer.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import umap\n",
    "\n",
    "from key_generation.dimensionality_reduction.base_reducer import DimensionalityReducer\n",
    "\n",
    "class UMAPReducer(DimensionalityReducer):\n",
    "    \"\"\"UMAP-based dimensionality reduction\"\"\"\n",
    "\n",
    "    def __init__(self, target_dims=32, random_state=42, n_neighbors=15, min_dist=0.1):\n",
    "        \"\"\"\n",
    "        Initialize UMAP reducer\n",
    "\n",
    "        Args:\n",
    "            target_dims: Target dimensionality for reduction\n",
    "            random_state: Random seed for reproducibility\n",
    "            n_neighbors: Number of neighbors for UMAP\n",
    "            min_dist: Minimum distance between points in the reduced space\n",
    "        \"\"\"\n",
    "        super().__init__(target_dims=target_dims, random_state=random_state)\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_dist = min_dist\n",
    "\n",
    "    def fit(self, embeddings):\n",
    "        \"\"\"\n",
    "        Fit UMAP model to embeddings\n",
    "\n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        # Initialize the UMAP model\n",
    "        self.model = umap.UMAP(\n",
    "            n_components=self.target_dims,\n",
    "            random_state=self.random_state,\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            min_dist=self.min_dist\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        self.model.fit(embeddings)\n",
    "        self.is_fitted = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, embeddings):\n",
    "        \"\"\"\n",
    "        Transform embeddings to lower dimension using UMAP\n",
    "\n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            reduced_embeddings: Array of reduced embeddings [n_samples, target_dims]\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"UMAP model is not fitted yet\")\n",
    "        \n",
    "        reduced_embeddings = self.model.transform(embeddings)\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the fitted UMAP model\n",
    "\n",
    "        Args:\n",
    "            path: Path to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"UMAP model is not fitted yet\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        joblib.dump(self.model, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"\n",
    "        Load a fitted UMAP model\n",
    "\n",
    "        Args:\n",
    "            path: Path to load the model from\n",
    "            \n",
    "        Returns:\n",
    "            reducer: Loaded UMAP reducer\n",
    "        \"\"\"\n",
    "        # Load model\n",
    "        model = joblib.load(path)\n",
    "        \n",
    "        # Create reducer instance\n",
    "        reducer = cls(\n",
    "            target_dims=model.n_components,\n",
    "            random_state=model.random_state,\n",
    "            n_neighbors=model.n_neighbors,\n",
    "            min_dist=model.min_dist\n",
    "        )\n",
    "        \n",
    "        # Restore model state\n",
    "        reducer.model = model\n",
    "        reducer.is_fitted = True\n",
    "        \n",
    "        return reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/dimensionality_reduction/tsne_reducer.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import joblib\n",
    "\n",
    "from key_generation.dimensionality_reduction.base_reducer import DimensionalityReducer\n",
    "\n",
    "class TSNEReducer(DimensionalityReducer):\n",
    "    \"\"\"t-SNE based dimensionality reduction\"\"\"\n",
    "    \n",
    "    def __init__(self, target_dims=2, random_state=42, perplexity=30, n_iter=1000):\n",
    "        \"\"\"\n",
    "        Initialize t-SNE reducer\n",
    "        \n",
    "        Args:\n",
    "            target_dims: Target dimensionality for reduction\n",
    "            random_state: Random seed for reproducibility\n",
    "            perplexity: Perplexity parameter for t-SNE\n",
    "            n_iter: Number of iterations for optimization\n",
    "        \"\"\"\n",
    "        super().__init__(target_dims=target_dims, random_state=random_state)\n",
    "        self.perplexity = perplexity\n",
    "        self.n_iter = n_iter\n",
    "    \n",
    "    def fit(self, embeddings):\n",
    "        \"\"\"\n",
    "        Fit t-SNE model to embeddings\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        # Initialize the t-SNE model\n",
    "        self.model = TSNE(\n",
    "            n_components=self.target_dims,\n",
    "            perplexity=self.perplexity,\n",
    "            n_iter=self.n_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        self.model.fit(embeddings)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, embeddings):\n",
    "        \"\"\"\n",
    "        Transform embeddings to lower dimension using t-SNE\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            reduced_embeddings: Array of reduced embeddings [n_samples, target_dims]\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"t-SNE model is not fitted yet\")\n",
    "        \n",
    "        reduced_embeddings = self.model.fit_transform(embeddings)\n",
    "        return reduced_embeddings\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the fitted t-SNE model\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"t-SNE model is not fitted yet\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        joblib.dump(self.model, path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"\n",
    "        Load a fitted t-SNE model\n",
    "        \n",
    "        Args:\n",
    "            path: Path to load the model from\n",
    "            \n",
    "        Returns:\n",
    "            reducer: Loaded t-SNE reducer\n",
    "        \"\"\"\n",
    "        # Load model\n",
    "        model = joblib.load(path)\n",
    "        \n",
    "        # Create reducer instance\n",
    "        reducer = cls(\n",
    "            target_dims=model.n_components,\n",
    "            random_state=model.random_state,\n",
    "            perplexity=model.perplexity,\n",
    "            n_iter=model.n_iter\n",
    "        )\n",
    "        \n",
    "        # Restore model state\n",
    "        reducer.model = model\n",
    "        reducer.is_fitted = True\n",
    "        \n",
    "        return reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9fda79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/dimensionality_reduction/reducer_factory.py\n",
    "\n",
    "from key_generation.dimensionality_reduction.pca_reducer import PCAReducer\n",
    "from key_generation.dimensionality_reduction.tsne_reducer import TSNEReducer\n",
    "from key_generation.dimensionality_reduction.umap_reducer import UMAPReducer\n",
    "from key_generation.dimensionality_reduction.random_projection_reducer import RandomProjectionReducer\n",
    "from key_generation.dimensionality_reduction.autoencoder_reducer import AutoencoderReducer\n",
    "\n",
    "def get_reducer(method, target_dims=32, **kwargs):\n",
    "    \"\"\"\n",
    "    Factory function to get a dimensionality reducer\n",
    "    \n",
    "    Args:\n",
    "        method: Reduction method ('pca', 'tsne', 'umap', 'random_projection', 'autoencoder')\n",
    "        target_dims: Target dimensionality for reduction\n",
    "        **kwargs: Additional arguments for specific reducers\n",
    "        \n",
    "    Returns:\n",
    "        reducer: Dimensionality reducer\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        return PCAReducer(target_dims=target_dims, **kwargs)\n",
    "    elif method == 'tsne':\n",
    "        return TSNEReducer(target_dims=target_dims, **kwargs)\n",
    "    elif method == 'umap':\n",
    "        return UMAPReducer(target_dims=target_dims, **kwargs)\n",
    "    elif method == 'random_projection':\n",
    "        return RandomProjectionReducer(target_dims=target_dims, **kwargs)\n",
    "    elif method == 'autoencoder':\n",
    "        return AutoencoderReducer(target_dims=target_dims, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reduction method: {method}\")\n",
    "\n",
    "def load_reducer(method, path):\n",
    "    \"\"\"\n",
    "    Load a fitted reducer\n",
    "    \n",
    "    Args:\n",
    "        method: Reduction method\n",
    "        path: Path to load the model from\n",
    "        \n",
    "    Returns:\n",
    "        reducer: Loaded dimensionality reducer\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        return PCAReducer.load(path)\n",
    "    elif method == 'tsne':\n",
    "        return TSNEReducer.load(path)\n",
    "    elif method == 'umap':\n",
    "        return UMAPReducer.load(path)\n",
    "    elif method == 'random_projection':\n",
    "        return RandomProjectionReducer.load(path)\n",
    "    elif method == 'autoencoder':\n",
    "        return AutoencoderReducer.load(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reduction method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51622aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/dimensionality_reduction/compare_reducers.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import trustworthiness\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from key_generation.dimensionality_reduction.reducer_factory import get_reducer\n",
    "\n",
    "def compare_reducers(embeddings, methods=None, target_dims=32, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Compare different dimensionality reduction methods\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Array of face embeddings [n_samples, n_features]\n",
    "        methods: List of reduction methods to compare\n",
    "        target_dims: Target dimensionality\n",
    "        n_neighbors: Number of neighbors for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        results: DataFrame with comparison results\n",
    "    \"\"\"\n",
    "    if methods is None:\n",
    "        methods = ['pca', 'umap', 'random_projection']\n",
    "    \n",
    "    # Initialize result metrics\n",
    "    results = {\n",
    "        'Method': [],\n",
    "        'Neighbor Preservation (%)': [],\n",
    "        'Trustworthiness': [],\n",
    "        'Silhouette Score': [],\n",
    "        'Processing Time (ms)': []\n",
    "    }\n",
    "    \n",
    "    # Precompute neighbors in original space\n",
    "    nn_original = NearestNeighbors(n_neighbors=n_neighbors+1)  # +1 because first match is self\n",
    "    nn_original.fit(embeddings)\n",
    "    _, indices_original = nn_original.kneighbors(embeddings)\n",
    "    indices_original = indices_original[:, 1:]  # Remove self match\n",
    "    \n",
    "    # Compare each method\n",
    "    for method in methods:\n",
    "        print(f\"Evaluating {method}...\")\n",
    "        \n",
    "        # Get reducer\n",
    "        reducer = get_reducer(method, target_dims=target_dims)\n",
    "        \n",
    "        # Measure time\n",
    "        start_time = time.time()\n",
    "        reduced = reducer.fit_transform(embeddings)\n",
    "        processing_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        \n",
    "        # Calculate neighbor preservation\n",
    "        nn_reduced = NearestNeighbors(n_neighbors=n_neighbors+1)\n",
    "        nn_reduced.fit(reduced)\n",
    "        _, indices_reduced = nn_reduced.kneighbors(reduced)\n",
    "        indices_reduced = indices_reduced[:, 1:]  # Remove self match\n",
    "        \n",
    "        # Calculate preservation rate\n",
    "        neighbor_preservation = 0\n",
    "        for i in range(len(embeddings)):\n",
    "            intersection = np.intersect1d(indices_original[i], indices_reduced[i])\n",
    "            neighbor_preservation += len(intersection) / n_neighbors\n",
    "        \n",
    "        neighbor_preservation = 100 * neighbor_preservation / len(embeddings)\n",
    "        \n",
    "        # Calculate trustworthiness\n",
    "        trust = trustworthiness(embeddings, reduced, n_neighbors=n_neighbors)\n",
    "        \n",
    "        # Calculate silhouette score (clustering quality)\n",
    "        # Use fewer clusters if samples are limited\n",
    "        n_clusters = min(5, len(embeddings) // 10)\n",
    "        if n_clusters >= 2:  # Need at least 2 clusters\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            clusters = kmeans.fit_predict(reduced)\n",
    "            silhouette = silhouette_score(reduced, clusters)\n",
    "        else:\n",
    "            silhouette = float('nan')\n",
    "        \n",
    "        # Store results\n",
    "        results['Method'].append(method)\n",
    "        results['Neighbor Preservation (%)'].append(round(neighbor_preservation, 2))\n",
    "        results['Trustworthiness'].append(round(trust, 4))\n",
    "        results['Silhouette Score'].append(round(silhouette, 4))\n",
    "        results['Processing Time (ms)'].append(round(processing_time, 2))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/similarity_component/base_lsh.py\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class LocalitySensitiveHash(ABC):\n",
    "    \"\"\"Base class for all LSH implementations\"\"\"\n",
    "    \n",
    "    def __init__(self, hash_bits=192, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the LSH hasher\n",
    "        \n",
    "        Args:\n",
    "            hash_bits: Number of bits in the output hash\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.hash_bits = hash_bits\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, embeddings):\n",
    "        \"\"\"\n",
    "        Fit the LSH model to the data\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def hash(self, embeddings):\n",
    "        \"\"\"\n",
    "        Generate hashes for the given embeddings\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            hashes: Binary array of shape [n_samples, hash_bits]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the fitted LSH model\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"LSH model is not fitted yet\")\n",
    "        \n",
    "        # Implement in subclasses\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"\n",
    "        Load a fitted LSH model\n",
    "        \n",
    "        Args:\n",
    "            path: Path to load the model from\n",
    "            \n",
    "        Returns:\n",
    "            lsh: Loaded LSH model\n",
    "        \"\"\"\n",
    "        # Implement in subclasses\n",
    "        pass\n",
    "    \n",
    "    def hamming_distance(self, hash1, hash2):\n",
    "        \"\"\"\n",
    "        Calculate Hamming distance between two hashes\n",
    "        \n",
    "        Args:\n",
    "            hash1: First hash (binary array)\n",
    "            hash2: Second hash (binary array)\n",
    "            \n",
    "        Returns:\n",
    "            distance: Hamming distance (number of different bits)\n",
    "        \"\"\"\n",
    "        return np.sum(hash1 != hash2)\n",
    "    \n",
    "    def hamming_similarity(self, hash1, hash2):\n",
    "        \"\"\"\n",
    "        Calculate Hamming similarity between two hashes\n",
    "        \n",
    "        Args:\n",
    "            hash1: First hash (binary array)\n",
    "            hash2: Second hash (binary array)\n",
    "            \n",
    "        Returns:\n",
    "            similarity: Hamming similarity (0-1, where 1 is identical)\n",
    "        \"\"\"\n",
    "        return 1 - (self.hamming_distance(hash1, hash2) / self.hash_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae02c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/similarity_component/simhash.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from key_generation.similarity_component.base_lsh import LocalitySensitiveHash\n",
    "\n",
    "class SimHash(LocalitySensitiveHash):\n",
    "    \"\"\"SimHash implementation for similarity-preserving hashing\"\"\"\n",
    "    \n",
    "    def __init__(self, hash_bits=192, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize SimHash\n",
    "        \n",
    "        Args:\n",
    "            hash_bits: Number of bits in the output hash\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        super().__init__(hash_bits=hash_bits, random_state=random_state)\n",
    "        \n",
    "        # Set random seed\n",
    "        np.random.seed(self.random_state)\n",
    "    \n",
    "    def fit(self, embeddings):\n",
    "        \"\"\"\n",
    "        Fit SimHash by generating random hyperplanes\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        # Get embedding dimension\n",
    "        self.input_dim = embeddings.shape[1]\n",
    "        \n",
    "        # Generate random hyperplanes\n",
    "        self.hyperplanes = np.random.randn(self.hash_bits, self.input_dim)\n",
    "        \n",
    "        # Normalize hyperplanes\n",
    "        self.hyperplanes = self.hyperplanes / np.linalg.norm(self.hyperplanes, axis=1, keepdims=True)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def hash(self, embeddings):\n",
    "        \"\"\"\n",
    "        Generate binary hashes using SimHash\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            hashes: Binary array of shape [n_samples, hash_bits]\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"SimHash model is not fitted yet\")\n",
    "        \n",
    "        # Project embeddings onto hyperplanes\n",
    "        projections = np.dot(embeddings, self.hyperplanes.T)\n",
    "        \n",
    "        # Convert to binary (0 or 1)\n",
    "        binary_hash = (projections > 0).astype(np.int8)\n",
    "        \n",
    "        return binary_hash\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the fitted SimHash model\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save the model\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"SimHash model is not fitted yet\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Save model data\n",
    "        model_data = {\n",
    "            'hyperplanes': self.hyperplanes,\n",
    "            'input_dim': self.input_dim,\n",
    "            'hash_bits': self.hash_bits,\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"\n",
    "        Load a fitted SimHash model\n",
    "        \n",
    "        Args:\n",
    "            path: Path to load the model from\n",
    "            \n",
    "        Returns:\n",
    "            simhash: Loaded SimHash model\n",
    "        \"\"\"\n",
    "        # Load model data\n",
    "        model_data = joblib.load(path)\n",
    "        \n",
    "        # Create model instance\n",
    "        simhash = cls(\n",
    "            hash_bits=model_data['hash_bits'],\n",
    "            random_state=model_data['random_state']\n",
    "        )\n",
    "        \n",
    "        # Restore model state\n",
    "        simhash.hyperplanes = model_data['hyperplanes']\n",
    "        simhash.input_dim = model_data['input_dim']\n",
    "        simhash.is_fitted = True\n",
    "        \n",
    "        return simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/similarity_component/lsh_factory.py\n",
    "\n",
    "from key_generation.similarity_component.simhash import SimHash\n",
    "from key_generation.similarity_component.spherical_lsh import SphericalLSH\n",
    "\n",
    "def get_lsh(method, hash_bits=192, **kwargs):\n",
    "    \"\"\"\n",
    "    Factory function to get an LSH hasher\n",
    "    \n",
    "    Args:\n",
    "        method: LSH method ('simhash', 'spherical')\n",
    "        hash_bits: Number of bits in the output hash\n",
    "        **kwargs: Additional arguments for specific hashers\n",
    "        \n",
    "    Returns:\n",
    "        lsh: LSH hasher\n",
    "    \"\"\"\n",
    "    if method == 'simhash':\n",
    "        return SimHash(hash_bits=hash_bits, **kwargs)\n",
    "    elif method == 'spherical':\n",
    "        return SphericalLSH(hash_bits=hash_bits, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown LSH method: {method}\")\n",
    "\n",
    "def load_lsh(method, path):\n",
    "    \"\"\"\n",
    "    Load a fitted LSH hasher\n",
    "    \n",
    "    Args:\n",
    "        method: LSH method\n",
    "        path: Path to load the model from\n",
    "        \n",
    "    Returns:\n",
    "        lsh: Loaded LSH hasher\n",
    "    \"\"\"\n",
    "    if method == 'simhash':\n",
    "        return SimHash.load(path)\n",
    "    elif method == 'spherical':\n",
    "        return SphericalLSH.load(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown LSH method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95792a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/security_component/metadata_hasher.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import joblib\n",
    "\n",
    "class MetadataHasher:\n",
    "    \"\"\"Generate security component from facial metadata\"\"\"\n",
    "    \n",
    "    def __init__(self, hash_bits=64, metadata_weights=None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize metadata hasher\n",
    "        \n",
    "        Args:\n",
    "            hash_bits: Number of bits in the output hash\n",
    "            metadata_weights: Dictionary of weights for different metadata fields\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.hash_bits = hash_bits\n",
    "        self.metadata_weights = metadata_weights\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Set default weights if not provided\n",
    "        if self.metadata_weights is None:\n",
    "            self.metadata_weights = self._get_default_weights()\n",
    "        \n",
    "        # Set random seed\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        # Features used for hashing\n",
    "        self.selected_features = None\n",
    "        self.normalized_weights = None\n",
    "    \n",
    "    def _get_default_weights(self):\n",
    "        \"\"\"Get default weights for metadata fields\"\"\"\n",
    "        return {\n",
    "            # Geometric features (higher weight)\n",
    "            'inter_ocular_ratio': 1.0,\n",
    "            'eye_aspect_ratio': 1.0,\n",
    "            'nose_width_ratio': 1.0,\n",
    "            'lip_fullness_ratio': 1.0,\n",
    "            'face_width_height_ratio': 1.0,\n",
    "            'jaw_angle': 1.0,\n",
    "            'face_symmetry': 0.5\n",
    "            # Other features would be included in a full implementation\n",
    "        }\n",
    "    \n",
    "    def fit(self, metadata_list):\n",
    "        \"\"\"\n",
    "        Analyze metadata to determine which features to use\n",
    "        \n",
    "        Args:\n",
    "            metadata_list: List of facial metadata dictionaries\n",
    "        \"\"\"\n",
    "        # Find consistently available features\n",
    "        if not metadata_list:\n",
    "            raise ValueError(\"Need at least one metadata sample to fit\")\n",
    "        \n",
    "        # Count availability of each feature\n",
    "        feature_counts = {}\n",
    "        for metadata in metadata_list:\n",
    "            for key in metadata:\n",
    "                if key in self.metadata_weights:\n",
    "                    feature_counts[key] = feature_counts.get(key, 0) + 1\n",
    "        \n",
    "        # Select features that are available in at least 90% of samples\n",
    "        threshold = 0.9 * len(metadata_list)\n",
    "        self.selected_features = [\n",
    "            feature for feature, count in feature_counts.items()\n",
    "            if count >= threshold and feature in self.metadata_weights\n",
    "        ]\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = sum(self.metadata_weights[f] for f in self.selected_features)\n",
    "        self.normalized_weights = {\n",
    "            f: self.metadata_weights[f] / total_weight \n",
    "            for f in self.selected_features\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def normalize_metadata(self, metadata):\n",
    "        \"\"\"\n",
    "        Normalize metadata values to [0, 1] range\n",
    "        \n",
    "        Args:\n",
    "            metadata: Dictionary of facial metadata\n",
    "            \n",
    "        Returns:\n",
    "            normalized: Dictionary of normalized metadata\n",
    "        \"\"\"\n",
    "        # Simple min-max normalization within expected ranges\n",
    "        normalized = {}\n",
    "        \n",
    "        for feature in self.selected_features:\n",
    "            if feature in metadata:\n",
    "                # Different normalization for different feature types\n",
    "                if 'ratio' in feature:\n",
    "                    # Ratios typically have known bounds\n",
    "                    normalized[feature] = min(1.0, max(0.0, metadata[feature] / 2.0))\n",
    "                elif 'angle' in feature:\n",
    "                    # Angles normalized to [0, 1]\n",
    "                    normalized[feature] = (metadata[feature] % 360) / 360.0\n",
    "                else:\n",
    "                    # Default normalization (assume 0-1 already)\n",
    "                    normalized[feature] = min(1.0, max(0.0, metadata[feature]))\n",
    "            else:\n",
    "                # If feature is missing, use median value\n",
    "                normalized[feature] = 0.5\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def hash(self, metadata):\n",
    "        \"\"\"\n",
    "        Generate a hash from metadata\n",
    "        \n",
    "        Args:\n",
    "            metadata: Dictionary of facial metadata\n",
    "            \n",
    "        Returns:\n",
    "            binary_hash: Binary array of shape [hash_bits]\n",
    "        \"\"\"\n",
    "        if self.selected_features is None:\n",
    "            raise ValueError(\"MetadataHasher needs to be fitted first\")\n",
    "        \n",
    "        # Normalize metadata\n",
    "        normalized = self.normalize_metadata(metadata)\n",
    "        \n",
    "        # Create a weighted string representation\n",
    "        feature_strings = []\n",
    "        for feature in self.selected_features:\n",
    "            # Weight the feature by repeating it proportionally to its weight\n",
    "            repetitions = max(1, int(self.normalized_weights[feature] * 10))\n",
    "            value_str = f\"{feature}:{normalized[feature]:.6f}\"\n",
    "            feature_strings.extend([value_str] * repetitions)\n",
    "        \n",
    "        # Sort for determinism and join\n",
    "        feature_strings.sort()\n",
    "        metadata_str = \"|\".join(feature_strings)\n",
    "        \n",
    "        # Generate SHA-256 hash\n",
    "        hasher = hashlib.sha256()\n",
    "        hasher.update(metadata_str.encode('utf-8'))\n",
    "        hash_bytes = hasher.digest()\n",
    "        \n",
    "        # Convert to binary array (take only the bits we need)\n",
    "        binary_hash = np.unpackbits(np.frombuffer(hash_bytes, dtype=np.uint8))[:self.hash_bits]\n",
    "        \n",
    "        return binary_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/face_key_generator.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "from key_generation.dimensionality_reduction.reducer_factory import get_reducer\n",
    "from key_generation.similarity_component.lsh_factory import get_lsh\n",
    "from key_generation.security_component.metadata_hasher import MetadataHasher\n",
    "\n",
    "class FacialPublicKeyGenerator:\n",
    "    \"\"\"Generate facial public keys from face embeddings and metadata\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        reduction_method='pca',\n",
    "        reduction_dims=32,\n",
    "        lsh_method='spherical',\n",
    "        similarity_bits=192,\n",
    "        security_bits=64,\n",
    "        random_state=42\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the key generator\n",
    "        \n",
    "        Args:\n",
    "            reduction_method: Method for dimensionality reduction\n",
    "            reduction_dims: Target dimensionality after reduction\n",
    "            lsh_method: Method for locality-sensitive hashing\n",
    "            similarity_bits: Number of bits for similarity component\n",
    "            security_bits: Number of bits for security component\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.reduction_method = reduction_method\n",
    "        self.reduction_dims = reduction_dims\n",
    "        self.lsh_method = lsh_method\n",
    "        self.similarity_bits = similarity_bits\n",
    "        self.security_bits = security_bits\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize components\n",
    "        self.reducer = get_reducer(\n",
    "            method=reduction_method,\n",
    "            target_dims=reduction_dims,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        self.lsh = get_lsh(\n",
    "            method=lsh_method,\n",
    "            hash_bits=similarity_bits,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        self.metadata_hasher = MetadataHasher(\n",
    "            hash_bits=security_bits,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, embeddings, metadata_list):\n",
    "        \"\"\"\n",
    "        Fit the key generator to training data\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of face embeddings [n_samples, n_features]\n",
    "            metadata_list: List of facial metadata dictionaries\n",
    "        \"\"\"\n",
    "        # Fit dimensionality reducer\n",
    "        print(f\"Fitting {self.reduction_method} reducer...\")\n",
    "        self.reducer.fit(embeddings)\n",
    "        \n",
    "        # Reduce dimensionality\n",
    "        reduced_embeddings = self.reducer.transform(embeddings)\n",
    "        \n",
    "        # Fit LSH hasher\n",
    "        print(f\"Fitting {self.lsh_method} hasher...\")\n",
    "        self.lsh.fit(reduced_embeddings)\n",
    "        \n",
    "        # Fit metadata hasher\n",
    "        print(\"Fitting metadata hasher...\")\n",
    "        self.metadata_hasher.fit(metadata_list)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate_key(self, embedding, metadata):\n",
    "        \"\"\"\n",
    "        Generate a facial public key\n",
    "        \n",
    "        Args:\n",
    "            embedding: Face embedding vector\n",
    "            metadata: Dictionary of facial metadata\n",
    "            \n",
    "        Returns:\n",
    "            key: Binary key array\n",
    "            similarity_component: Similarity component of the key\n",
    "            security_component: Security component of the key\n",
    "            generation_time_ms: Key generation time in milliseconds\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Key generator needs to be fitted first\")\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Reshape embedding if needed\n",
    "        embedding = np.atleast_2d(embedding)\n",
    "        \n",
    "        # Reduce dimensionality\n",
    "        reduced_embedding = self.reducer.transform(embedding)\n",
    "        \n",
    "        # Generate similarity component\n",
    "        similarity_component = self.lsh.hash(reduced_embedding)[0]\n",
    "        \n",
    "        # Generate security component\n",
    "        security_component = self.metadata_hasher.hash(metadata)\n",
    "        \n",
    "        # Combine components\n",
    "        key = np.concatenate([similarity_component, security_component])\n",
    "        \n",
    "        # Measure generation time\n",
    "        generation_time_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return key, similarity_component, security_component, generation_time_ms\n",
    "    \n",
    "    def key_to_hex(self, key):\n",
    "        \"\"\"\n",
    "        Convert binary key to hexadecimal string\n",
    "        \n",
    "        Args:\n",
    "            key: Binary key array\n",
    "            \n",
    "        Returns:\n",
    "            hex_key: Hexadecimal string\n",
    "        \"\"\"\n",
    "        # Pack bits into bytes\n",
    "        key_bytes = np.packbits(key)\n",
    "        \n",
    "        # Convert to hex string\n",
    "        hex_key = ''.join(f\"{b:02x}\" for b in key_bytes)\n",
    "        \n",
    "        return hex_key\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d68e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_keys(self, key1, key2):\n",
    "    \"\"\"\n",
    "    Compare two keys and return similarity\n",
    "    \n",
    "    Args:\n",
    "        key1: First binary key array\n",
    "        key2: Second binary key array\n",
    "        \n",
    "    Returns:\n",
    "        similarity: Overall similarity (0-1)\n",
    "        sim_component_similarity: Similarity component similarity (0-1)\n",
    "        sec_component_match: Security component exact match (True/False)\n",
    "    \"\"\"\n",
    "    # Extract components\n",
    "    sim_component1 = key1[:self.similarity_bits]\n",
    "    sec_component1 = key1[self.similarity_bits:]\n",
    "    \n",
    "    sim_component2 = key2[:self.similarity_bits]\n",
    "    sec_component2 = key2[self.similarity_bits:]\n",
    "    \n",
    "    # Calculate similarity component similarity\n",
    "    sim_component_similarity = 1 - (np.sum(sim_component1 != sim_component2) / self.similarity_bits)\n",
    "    \n",
    "    # Check if security components match\n",
    "    sec_component_match = np.array_equal(sec_component1, sec_component2)\n",
    "    \n",
    "    # Calculate overall similarity\n",
    "    sim_weight = self.similarity_bits / (self.similarity_bits + self.security_bits)\n",
    "    sec_weight = self.security_bits / (self.similarity_bits + self.security_bits)\n",
    "    \n",
    "    security_score = 1.0 if sec_component_match else 0.0\n",
    "    \n",
    "    similarity = (sim_weight * sim_component_similarity) + (sec_weight * security_score)\n",
    "    \n",
    "    return similarity, sim_component_similarity, sec_component_match\n",
    "\n",
    "def find_similar_keys(self, target_key, key_database, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Find similar keys in a database\n",
    "    \n",
    "    Args:\n",
    "        target_key: Target key to search for\n",
    "        key_database: Dictionary mapping IDs to keys\n",
    "        threshold: Similarity threshold (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        matches: List of (id, similarity) tuples for matches above threshold\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    for key_id, key in key_database.items():\n",
    "        similarity, _, _ = self.compare_keys(target_key, key)\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            matches.append((key_id, similarity))\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    matches.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def evaluate_key_quality(self, embeddings, metadata_list, same_person_pairs=None, diff_person_pairs=None):\n",
    "    \"\"\"\n",
    "    Evaluate key quality with similarity metrics\n",
    "    \n",
    "    Args:\n",
    "        embeddings: List of face embeddings\n",
    "        metadata_list: List of facial metadata dictionaries\n",
    "        same_person_pairs: List of (idx1, idx2) tuples for same person\n",
    "        diff_person_pairs: List of (idx1, idx2) tuples for different people\n",
    "        \n",
    "    Returns:\n",
    "        results: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    if not self.is_fitted:\n",
    "        raise ValueError(\"Key generator needs to be fitted first\")\n",
    "    \n",
    "    # Generate keys for all faces\n",
    "    keys = []\n",
    "    for embedding, metadata in zip(embeddings, metadata_list):\n",
    "        key, _, _, _ = self.generate_key(embedding, metadata)\n",
    "        keys.append(key)\n",
    "    \n",
    "    # If pairs not provided, evaluate all possible pairs\n",
    "    if same_person_pairs is None and diff_person_pairs is None:\n",
    "        print(\"No specific pairs provided; evaluating random sample of all pairs\")\n",
    "        \n",
    "        # Generate random pairs for evaluation\n",
    "        np.random.seed(self.random_state)\n",
    "        all_indices = np.arange(len(keys))\n",
    "        \n",
    "        # For simplicity, assume first half are same person in different photos\n",
    "        # and second half are different people\n",
    "        same_indices = all_indices[:len(all_indices)//2]\n",
    "        diff_indices = all_indices[len(all_indices)//2:]\n",
    "        \n",
    "        # Generate up to 100 same-person pairs\n",
    "        same_person_pairs = []\n",
    "        for i in range(min(100, len(same_indices) * (len(same_indices) - 1) // 2)):\n",
    "            idx1, idx2 = np.random.choice(same_indices, 2, replace=False)\n",
    "            same_person_pairs.append((idx1, idx2))\n",
    "        \n",
    "        # Generate up to 100 different-person pairs\n",
    "        diff_person_pairs = []\n",
    "        for i in range(min(100, len(diff_indices) * (len(diff_indices) - 1) // 2)):\n",
    "            idx1, idx2 = np.random.choice(diff_indices, 2, replace=False)\n",
    "            diff_person_pairs.append((idx1, idx2))\n",
    "    \n",
    "    # Evaluate same-person pairs\n",
    "    same_person_similarities = []\n",
    "    for idx1, idx2 in same_person_pairs:\n",
    "        similarity, _, _ = self.compare_keys(keys[idx1], keys[idx2])\n",
    "        same_person_similarities.append(similarity)\n",
    "    \n",
    "    # Evaluate different-person pairs\n",
    "    diff_person_similarities = []\n",
    "    for idx1, idx2 in diff_person_pairs:\n",
    "        similarity, _, _ = self.compare_keys(keys[idx1], keys[idx2])\n",
    "        diff_person_similarities.append(similarity)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'same_person_mean': np.mean(same_person_similarities),\n",
    "        'same_person_std': np.std(same_person_similarities),\n",
    "        'diff_person_mean': np.mean(diff_person_similarities),\n",
    "        'diff_person_std': np.std(diff_person_similarities),\n",
    "        'separation': np.mean(same_person_similarities) - np.mean(diff_person_similarities)\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972dc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_generation/visualization/key_visualizer.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_key_bits(key, similarity_bits=192, figsize=(10, 2)):\n",
    "    \"\"\"\n",
    "    Visualize a key's bit pattern\n",
    "    \n",
    "    Args:\n",
    "        key: Binary key array\n",
    "        similarity_bits: Number of bits in similarity component\n",
    "        figsize: Figure size\n",
    "        \n",
    "    Returns:\n",
    "        fig: Matplotlib figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Reshape for visualization\n",
    "    key_2d = key.reshape(1, -1)\n",
    "    \n",
    "    # Create a colormap with white (0) and blue (1)\n",
    "    cmap = plt.cm.Blues\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(key_2d, cmap=cmap, cbar=False, ax=ax, \n",
    "                xticklabels=False, yticklabels=False)\n",
    "    \n",
    "    # Add vertical line to separate similarity and security components\n",
    "    if similarity_bits < len(key):\n",
    "        ax.axvline(x=similarity_bits, color='red', linestyle='--', linewidth=2)\n",
    "        \n",
    "        # Add labels\n",
    "        mid_sim = similarity_bits // 2\n",
    "        mid_sec = similarity_bits + (len(key) - similarity_bits) // 2\n",
    "        \n",
    "        ax.text(mid_sim, -0.3, \"Similarity Component\", ha='center', fontsize=10)\n",
    "        ax.text(mid_sec, -0.3, \"Security Component\", ha='center', fontsize=10)\n",
    "    \n",
    "    ax.set_title(\"Facial Public Key Bit Pattern\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_key_similarities(same_person_similarities, diff_person_similarities, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plot histogram of key similarities for same person and different people\n",
    "    \n",
    "    Args:\n",
    "        same_person_similarities: List of similarities between same person\n",
    "        diff_person_similarities: List of similarities between different people\n",
    "        figsize: Figure size\n",
    "        \n",
    "    Returns:\n",
    "        fig: Matplotlib figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot histograms\n",
    "    sns.histplot(same_person_similarities, color='green', alpha=0.6, \n",
    "                 label='Same Person', kde=True, ax=ax)\n",
    "    sns.histplot(diff_person_similarities, color='red', alpha=0.6, \n",
    "                 label='Different People', kde=True, ax=ax)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Key Similarity (0-1)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Distribution of Key Similarities')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add vertical line at potential threshold\n",
    "    threshold = (np.mean(same_person_similarities) + np.mean(diff_person_similarities)) / 2\n",
    "    ax.axvline(x=threshold, color='blue', linestyle='--', linewidth=2,\n",
    "              label=f'Potential Threshold: {threshold:.2f}')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_roc_curve(same_person_similarities, diff_person_similarities, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plot ROC curve for key matching\n",
    "    \n",
    "    Args:\n",
    "        same_person_similarities: List of similarities between same person\n",
    "        diff_person_similarities: List of similarities between different people\n",
    "        figsize: Figure size\n",
    "        \n",
    "    Returns:\n",
    "        fig: Matplotlib figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create ground truth labels\n",
    "    y_true = np.hstack([np.ones(len(same_person_similarities)), \n",
    "                       np.zeros(len(diff_person_similarities))])\n",
    "    \n",
    "    # Create similarity scores\n",
    "    y_scores = np.hstack([same_person_similarities, diff_person_similarities])\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "            label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    \n",
    "    return fig, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Phase 1 components\n",
    "from main import process_image  # From Phase 1\n",
    "\n",
    "# Import Phase 2 components\n",
    "from key_generation.face_key_generator import FacialPublicKeyGenerator\n",
    "from key_generation.visualization.key_visualizer import plot_key_bits, plot_key_similarities\n",
    "\n",
    "def train_key_generator(dataset_path, output_model_path, reduction_method='pca', lsh_method='spherical'):\n",
    "    \"\"\"\n",
    "    Train the facial public key generator on a dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to dataset of Phase 1 results\n",
    "        output_model_path: Path to save the trained model\n",
    "        reduction_method: Method for dimensionality reduction\n",
    "        lsh_method: Method for locality-sensitive hashing\n",
    "        \n",
    "    Returns:\n",
    "        key_generator: Trained FacialPublicKeyGenerator\n",
    "    \"\"\"\n",
    "    print(f\"Training key generator with {reduction_method} and {lsh_method}...\")\n",
    "    \n",
    "    # Collect embeddings and metadata from dataset\n",
    "    embeddings = []\n",
    "    metadata_list = []\n",
    "    \n",
    "    # Load all files from dataset\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(dataset_path, filename)\n",
    "            \n",
    "            try:\n",
    "                # Load Phase 1 result\n",
    "                with open(file_path, 'r') as f:\n",
    "                    result = json.load(f)\n",
    "                \n",
    "                # Check if valid result\n",
    "                if 'embedding' in result and 'metadata' in result:\n",
    "                    embeddings.append(np.array(result['embedding']))\n",
    "                    metadata_list.append(result['metadata'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(embeddings)} samples from dataset\")\n",
    "    \n",
    "    if len(embeddings) < 10:\n",
    "        raise ValueError(\"Too few samples in dataset (need at least 10)\")\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Create and train key generator\n",
    "    key_generator = FacialPublicKeyGenerator(\n",
    "        reduction_method=reduction_method,\n",
    "        lsh_method=lsh_method\n",
    "    )\n",
    "    \n",
    "    # Fit the key generator\n",
    "    key_generator.fit(embeddings, metadata_list)\n",
    "    \n",
    "    # Save the model\n",
    "    os.makedirs(os.path.dirname(output_model_path), exist_ok=True)\n",
    "    key_generator.save(output_model_path)\n",
    "    \n",
    "    print(f\"Key generator trained and saved to {output_model_path}\")\n",
    "    \n",
    "    return key_generator\n",
    "\n",
    "def generate_key_from_image(image_path, key_generator, output_dir=None, visualize=False):\n",
    "    \"\"\"\n",
    "    Generate a facial public key from an image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        key_generator: Trained FacialPublicKeyGenerator\n",
    "        output_dir: Directory to save results\n",
    "        visualize: Whether to visualize the key\n",
    "        \n",
    "    Returns:\n",
    "        result: Dictionary with key generation results\n",
    "    \"\"\"\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    \n",
    "    # Process image through Phase 1\n",
    "    phase1_result = process_image(image_path)\n",
    "    \n",
    "    # Check for errors\n",
    "    if 'error' in phase1_result:\n",
    "        return {'error': phase1_result['error']}\n",
    "    \n",
    "    # Extract embedding and metadata\n",
    "    embedding = np.array(phase1_result['embedding'])\n",
    "    metadata = phase1_result['metadata']\n",
    "    \n",
    "    # Generate key\n",
    "    start_time = time.time()\n",
    "    key, sim_component, sec_component, _ = key_generator.generate_key(embedding, metadata)\n",
    "    key_generation_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "    \n",
    "    # Convert to hex for display\n",
    "    hex_key = key_generator.key_to_hex(key)\n",
    "    \n",
    "    # Prepare result\n",
    "    result = {\n",
    "        'input_image': image_path,\n",
    "        'binary_key': key.tolist(),\n",
    "        'hex_key': hex_key,\n",
    "        'generation_time_ms': key_generation_time,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save result if output directory provided\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Generate output filename\n",
    "        base_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        output_path = os.path.join(output_dir, f\"{base_filename}_key.json\")\n",
    "        \n",
    "        # Save JSON result\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        \n",
    "        # Visualize key if requested\n",
    "        if visualize:\n",
    "            fig = plot_key_bits(key, similarity_bits=key_generator.similarity_bits)\n",
    "            vis_path = os.path.join(output_dir, f\"{base_filename}_key_visualization.png\")\n",
    "            fig.savefig(vis_path)\n",
    "            plt.close(fig)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Facial Public Key Generation (Phase 2)')\n",
    "    \n",
    "    # Mode selection\n",
    "    parser.add_argument('mode', choices=['train', 'generate', 'batch'],\n",
    "                        help='Operation mode: train model, generate single key, or batch process')\n",
    "    \n",
    "    # Common parameters\n",
    "    parser.add_argument('--model', default='models/key_generator',\n",
    "                        help='Path to model directory (for saving or loading)')\n",
    "    \n",
    "    # Training parameters\n",
    "    parser.add_argument('--dataset', help='Path to training dataset (Phase 1 results)')\n",
    "    parser.add_argument('--reduction', default='pca', \n",
    "                        choices=['pca', 'umap', 'random_projection'],\n",
    "                        help='Dimensionality reduction method')\n",
    "    parser.add_argument('--lsh', default='spherical', \n",
    "                        choices=['simhash', 'spherical'],\n",
    "                        help='LSH method for similarity component')\n",
    "    \n",
    "    # Generation parameters\n",
    "    parser.add_argument('--image', help='Path to input image (for single key generation)')\n",
    "    parser.add_argument('--input-dir', help='Directory with input images (for batch processing)')\n",
    "    parser.add_argument('--output-dir', default='results',\n",
    "                        help='Directory to save results')\n",
    "    parser.add_argument('--visualize', action='store_true',\n",
    "                        help='Generate visualizations')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Operation based on mode\n",
    "    if args.mode == 'train':\n",
    "        if not args.dataset:\n",
    "            parser.error(\"Training mode requires --dataset\")\n",
    "        \n",
    "        # Train key generator\n",
    "        train_key_generator(\n",
    "            dataset_path=args.dataset,\n",
    "            output_model_path=args.model,\n",
    "            reduction_method=args.reduction,\n",
    "            lsh_method=args.lsh\n",
    "        )\n",
    "    \n",
    "    elif args.mode == 'generate':\n",
    "        if not args.image:\n",
    "            parser.error(\"Generate mode requires --image\")\n",
    "        \n",
    "        # Load key generator\n",
    "        key_generator = FacialPublicKeyGenerator.load(args.model)\n",
    "        \n",
    "        # Generate key from image\n",
    "        result = generate_key_from_image(\n",
    "            image_path=args.image,\n",
    "            key_generator=key_generator,\n",
    "            output_dir=args.output_dir,\n",
    "            visualize=args.visualize\n",
    "        )\n",
    "        \n",
    "        # Print result\n",
    "        if 'error' in result:\n",
    "            print(f\"Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"Generated key: {result['hex_key']}\")\n",
    "            print(f\"Generation time: {result['generation_time_ms']:.2f} ms\")\n",
    "    \n",
    "    elif args.mode == 'batch':\n",
    "        if not args.input_dir:\n",
    "            parser.error(\"Batch mode requires --input-dir\")\n",
    "        \n",
    "        # Load key generator\n",
    "        key_generator = FacialPublicKeyGenerator.load(args.model)\n",
    "        \n",
    "        # Process all images in directory\n",
    "        processed = 0\n",
    "        errors = 0\n",
    "        \n",
    "        for filename in os.listdir(args.input_dir):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_path = os.path.join(args.input_dir, filename)\n",
    "                \n",
    "                # Generate key\n",
    "                result = generate_key_from_image(\n",
    "                    image_path=image_path,\n",
    "                    key_generator=key_generator,\n",
    "                    output_dir=args.output_dir,\n",
    "                    visualize=args.visualize\n",
    "                )\n",
    "                \n",
    "                if 'error' in result:\n",
    "                    print(f\"Error processing {filename}: {result['error']}\")\n",
    "                    errors += 1\n",
    "                else:\n",
    "                    processed += 1\n",
    "        \n",
    "        print(f\"Batch processing complete: {processed} images processed, {errors} errors\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827dfb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_phase2.py\n",
    "\n",
    "import unittest\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "from key_generation.dimensionality_reduction.pca_reducer import PCAReducer\n",
    "from key_generation.similarity_component.simhash import SimHash\n",
    "from key_generation.security_component.metadata_hasher import MetadataHasher\n",
    "from key_generation.face_key_generator import FacialPublicKeyGenerator\n",
    "\n",
    "class TestPhase2(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        # Create temporary directory for test files\n",
    "        self.test_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Create sample data\n",
    "        np.random.seed(42)\n",
    "        self.sample_embeddings = np.random.randn(20, 128)\n",
    "        \n",
    "        self.sample_metadata = []\n",
    "        for i in range(20):\n",
    "            self.sample_metadata.append({\n",
    "                'inter_ocular_ratio': 0.43 + np.random.rand() * 0.1,\n",
    "                'eye_aspect_ratio': 0.3 + np.random.rand() * 0.2,\n",
    "                'nose_width_ratio': 0.25 + np.random.rand() * 0.1,\n",
    "                'face_width_height_ratio': 0.7 + np.random.rand() * 0.2,\n",
    "                'face_symmetry': 0.8 + np.random.rand() * 0.2\n",
    "            })\n",
    "    \n",
    "    def tearDown(self):\n",
    "        # Remove test directory\n",
    "        shutil.rmtree(self.test_dir)\n",
    "    \n",
    "    def test_pca_reducer(self):\n",
    "        # Test PCA reducer\n",
    "        reducer = PCAReducer(target_dims=32)\n",
    "        reducer.fit(self.sample_embeddings)\n",
    "        \n",
    "        # Test dimensionality reduction\n",
    "        reduced = reducer.transform(self.sample_embeddings)\n",
    "        self.assertEqual(reduced.shape, (20, 32))\n",
    "        \n",
    "        # Test save and load\n",
    "        save_path = os.path.join(self.test_dir, 'pca_model.pkl')\n",
    "        reducer.save(save_path)\n",
    "        \n",
    "        loaded_reducer = PCAReducer.load(save_path)\n",
    "        loaded_reduced = loaded_reducer.transform(self.sample_embeddings)\n",
    "        \n",
    "        # Check that results are the same\n",
    "        np.testing.assert_allclose(reduced, loaded_reduced)\n",
    "    \n",
    "    def test_simhash(self):\n",
    "        # Test SimHash\n",
    "        hasher = SimHash(hash_bits=192)\n",
    "        hasher.fit(self.sample_embeddings)\n",
    "        \n",
    "        # Test hashing\n",
    "        hashes = hasher.hash(self.sample_embeddings)\n",
    "        self.assertEqual(hashes.shape, (20, 192))\n",
    "        \n",
    "        # Test types\n",
    "        self.assertTrue(np.issubdtype(hashes.dtype, np.integer))\n",
    "        self.assertTrue(np.all((hashes == 0) | (hashes == 1)))\n",
    "        \n",
    "        # Test save and load\n",
    "        save_path = os.path.join(self.test_dir, 'simhash_model.pkl')\n",
    "        hasher.save(save_path)\n",
    "        \n",
    "        loaded_hasher = SimHash.load(save_path)\n",
    "        loaded_hashes = loaded_hasher.hash(self.sample_embeddings)\n",
    "        \n",
    "        # Check that results are the same\n",
    "        np.testing.assert_array_equal(hashes, loaded_hashes)\n",
    "    \n",
    "    def test_metadata_hasher(self):\n",
    "        # Test metadata hasher\n",
    "        hasher = MetadataHasher(hash_bits=64)\n",
    "        hasher.fit(self.sample_metadata)\n",
    "        \n",
    "        # Test hashing\n",
    "        hash1 = hasher.hash(self.sample_metadata[0])\n",
    "        self.assertEqual(hash1.shape, (64,))\n",
    "        \n",
    "        # Test determinism (same input should give same hash)\n",
    "        hash2 = hasher.hash(self.sample_metadata[0])\n",
    "        np.testing.assert_array_equal(hash1, hash2)\n",
    "        \n",
    "        # Different input should give different hash\n",
    "        hash3 = hasher.hash(self.sample_metadata[1])\n",
    "        self.assertFalse(np.array_equal(hash1, hash3))\n",
    "    \n",
    "    def test_face_key_generator(self):\n",
    "        # Test full key generator\n",
    "        key_generator = FacialPublicKeyGenerator(\n",
    "            reduction_method='pca',\n",
    "            lsh_method='simhash',\n",
    "            similarity_bits=192,\n",
    "            security_bits=64\n",
    "        )\n",
    "        \n",
    "        # Fit the generator\n",
    "        key_generator.fit(self.sample_embeddings, self.sample_metadata)\n",
    "        \n",
    "        # Generate key\n",
    "        key, sim_component, sec_component, _ = key_generator.generate_key(\n",
    "            self.sample_embeddings[0], self.sample_metadata[0]\n",
    "        )\n",
    "        \n",
    "        # Check dimensions\n",
    "        self.assertEqual(len(key), 256)\n",
    "        self.assertEqual(len(sim_component), 192)\n",
    "        self.assertEqual(len(sec_component), 64)\n",
    "        \n",
    "        # Test hex conversion\n",
    "        hex_key = key_generator.key_to_hex(key)\n",
    "        self.assertEqual(len(hex_key), 64)  # 256 bits = 32 bytes = 64 hex chars\n",
    "        \n",
    "        # Test key comparison\n",
    "        key2, _, _, _ = key_generator.generate_key(\n",
    "            self.sample_embeddings[0], self.sample_metadata[0]\n",
    "        )\n",
    "        similarity, sim_component_similarity, sec_component_match = key_generator.compare_keys(key, key2)\n",
    "        \n",
    "        # Same input should give same key\n",
    "        self.assertEqual(similarity, 1.0)\n",
    "        self.assertEqual(sim_component_similarity, 1.0)\n",
    "        self.assertTrue(sec_component_match)\n",
    "        \n",
    "        # Different faces should give different keys\n",
    "        key3, _, _, _ = key_generator.generate_key(\n",
    "            self.sample_embeddings[5], self.sample_metadata[5]\n",
    "        )\n",
    "        similarity3, _, _ = key_generator.compare_keys(key, key3)\n",
    "        self.assertLess(similarity3, 1.0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
